{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exog_params  {'UpStep': 1, 'DownStep': -1, 'Variance': 2, 'biasdf':           Up  Neutral  Down\n",
      "Up       0.9      0.1   0.0\n",
      "Neutral  0.2      0.6   0.2\n",
      "Down     0.0      0.1   0.9}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Asset selling driver script\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from AssetSellingModel_Q3 import AssetSellingModel\n",
    "from AssetSellingPolicy_Q3 import AssetSellingPolicy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "import time\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "\n",
    "\n",
    "# read in policy parameters from an Excel spreadsheet, \"asset_selling_policy_parameters.xlsx\"\n",
    "sheet1 = pd.read_excel(\"asset_selling_policy_parameters.xlsx\", sheet_name=\"Sheet1\")\n",
    "params = zip(sheet1[\"param1\"], sheet1[\"param2\"])\n",
    "param_list = list(params)\n",
    "sheet2 = pd.read_excel(\"asset_selling_policy_parameters.xlsx\", sheet_name=\"Sheet2\")\n",
    "sheet3 = pd.read_excel(\"asset_selling_policy_parameters.xlsx\", sheet_name=\"Sheet3\")\n",
    "biasdf = pd.read_excel(\n",
    "    \"asset_selling_policy_parameters.xlsx\", sheet_name=\"Sheet4\", index_col=0\n",
    ")\n",
    "\n",
    "policy_selected = sheet3[\"Policy\"][0]\n",
    "T = sheet3[\"TimeHorizon\"][0]\n",
    "gamma = sheet3[\"DiscountFactor\"][0]\n",
    "initPrice = sheet3[\"InitialPrice\"][0]\n",
    "initBias = sheet3[\"InitialBias\"][0]\n",
    "\n",
    "exog_params = {\n",
    "    \"UpStep\": sheet3[\"UpStep\"][0],\n",
    "    \"DownStep\": sheet3[\"DownStep\"][0],\n",
    "    \"Variance\": sheet3[\"Variance\"][0],\n",
    "    \"biasdf\": biasdf,\n",
    "}\n",
    "\n",
    "nIterations = sheet3[\"Iterations\"][0]\n",
    "printStep = sheet3[\"PrintStep\"][0]\n",
    "printIterations = [0]\n",
    "printIterations.extend(list(reversed(range(nIterations - 1, 0, -printStep))))\n",
    "\n",
    "print(\"exog_params \", exog_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected policy track, time horizon 40, initial price 16 and number of iterations 10000\n",
      "Theta 2, discount factor 0.99,  Current price 15.68577136512636, smoothed_price 16.05474204265753, d_smoothed_prince 16.05474204265753, and hold interval (14.05474204265753, 18.05474204265753)\n",
      "Theta 2, discount factor 0.9801,  Current price 12.813928171950211, smoothed_price 14.002610968929911, d_smoothed_prince 14.002610968929911, and hold interval (12.002610968929911, 16.00261096892991)\n",
      "Theta 2, discount factor 0.970299,  Current price 15.912197393726274, smoothed_price 15.678732437269922, d_smoothed_prince 15.678732437269922, and hold interval (13.678732437269922, 17.67873243726992)\n",
      "Theta 2, discount factor 0.96059601,  Current price 13.916843485441937, smoothed_price 14.72866686579304, d_smoothed_prince 14.72866686579304, and hold interval (12.72866686579304, 16.728666865793038)\n",
      "Theta 2, discount factor 0.9509900498999999,  Current price 16.151095376888527, smoothed_price 16.42588820160755, d_smoothed_prince 16.42588820160755, and hold interval (14.425888201607549, 18.42588820160755)\n",
      "Theta 2, discount factor 0.941480149401,  Current price 19.32705672251477, smoothed_price 19.215324329357777, d_smoothed_prince 19.215324329357777, and hold interval (17.215324329357777, 21.215324329357777)\n",
      "Theta 2, discount factor 0.9320653479069899,  Current price 16.70161696368658, smoothed_price 18.34727361330233, d_smoothed_prince 18.34727361330233, and hold interval (16.34727361330233, 20.34727361330233)\n",
      "Theta 2, discount factor 0.9227446944279201,  Current price 17.836632380287835, smoothed_price 19.167606989439804, d_smoothed_prince 19.167606989439804, and hold interval (17.167606989439804, 21.167606989439804)\n",
      "Theta 2, discount factor 0.9135172474836408,  Current price 15.28098491973061, smoothed_price 17.367234365220295, d_smoothed_prince 17.367234365220295, and hold interval (15.367234365220295, 19.367234365220295)\n",
      "obj=15.128175070533302, state.resource=0\n",
      "Iteration 0 for theta 2. The contribution was 15.128175070533302 and the stopping time was 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**************************************************************************************\n",
      "Finishing iterations for theta 2. Average contribution 15.128175070533302 and average stopping time 11.0\n",
      "0.0041201114654541016 secs\n"
     ]
    }
   ],
   "source": [
    "# initialize the model and the policy\n",
    "policy_names = [\"sell_low\", \"high_low\", \"track\"]\n",
    "state_names = [\"price\", \"resource\", \"bias\", \"prev_price\", \"prev_price2\"]\n",
    "init_state = {\n",
    "    \"price\": initPrice,\n",
    "    \"resource\": 1,\n",
    "    \"bias\": initBias,\n",
    "    \"prev_price\": initPrice,\n",
    "    \"prev_price2\": initPrice,\n",
    "}\n",
    "decision_names = [\"sell\", \"hold\"]\n",
    "\n",
    "\n",
    "M = AssetSellingModel(state_names, decision_names, init_state, exog_params, T, gamma)\n",
    "P = AssetSellingPolicy(M, policy_names)\n",
    "t = 0\n",
    "prev_price = init_state[\"price\"]\n",
    "\n",
    "\n",
    "# make a policy_info dict object\n",
    "policy_info = {\n",
    "    \"sell_low\": param_list[0],\n",
    "    \"high_low\": param_list[1],\n",
    "    \"track\": param_list[2] + (prev_price, prev_price),\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "print(\n",
    "    \"Selected policy {}, time horizon {}, initial price {} and number of iterations {}\".format(\n",
    "        policy_selected, T, initPrice, nIterations\n",
    "    )\n",
    ")\n",
    "\n",
    "if not policy_selected in [\"full_grid\", \"track\"]:\n",
    "    contribution_iterations = [\n",
    "        P.run_policy(param_list, policy_info, policy_selected, t)\n",
    "        for ite in list(range(nIterations))\n",
    "    ]\n",
    "    contribution_iterations = pd.Series(contribution_iterations)\n",
    "\n",
    "    print(\"Contribution per iteration: \")\n",
    "    print(contribution_iterations)\n",
    "    cum_avg_contrib = contribution_iterations.expanding().mean()\n",
    "\n",
    "    print(\"Cumulative average contribution per iteration: \")\n",
    "    print(cum_avg_contrib)\n",
    "\n",
    "    # plotting the results\n",
    "    fig, axsubs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    fig.suptitle(\n",
    "        \"Asset selling using policy {} with parameters {} and T {}\".format(\n",
    "            policy_selected, policy_info[policy_selected], T\n",
    "        )\n",
    "    )\n",
    "    i = np.arange(0, nIterations, 1)\n",
    "\n",
    "    axsubs[0].plot(i, cum_avg_contrib, \"g\")\n",
    "    axsubs[0].set_title(\"Cumulative average contribution\")\n",
    "\n",
    "    axsubs[1].plot(i, contribution_iterations, \"g\")\n",
    "    axsubs[1].set_title(\"Contribution per iteration\")\n",
    "\n",
    "    # Create a big subplot\n",
    "    ax = fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axes\n",
    "    plt.tick_params(labelcolor=\"none\", top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "    ax.set_ylabel(\"USD\", labelpad=0)  # Use argument `labelpad` to move label downwards.\n",
    "    ax.set_xlabel(\"Iterations\", labelpad=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "elif policy_selected == \"track\":\n",
    "    theta = policy_info[\"track\"][0]\n",
    "    policy_info[\"track\"] = (\n",
    "        theta,\n",
    "        None,\n",
    "        policy_info[\"track\"][2],\n",
    "        policy_info[\"track\"][3],\n",
    "    )\n",
    "    res = []\n",
    "    t_stop_arr = []\n",
    "    for k in range(1):\n",
    "        contrib, t_stop = P.run_policy(param_list, policy_info, policy_selected, t)\n",
    "        res.append(contrib)\n",
    "        t_stop_arr.append(t_stop)\n",
    "        print(\n",
    "            \"Iteration {} for theta {}. The contribution was {} and the stopping time was {}\".format(\n",
    "                k, theta, contrib, t_stop\n",
    "            )\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "    avg_contrib = np.array(res).mean()\n",
    "    avg_t_stop = np.array(t_stop_arr).mean()\n",
    "    print(\"\\n\")\n",
    "    print(\n",
    "        \"**************************************************************************************\"\n",
    "    )\n",
    "    print(\n",
    "        \"Finishing iterations for theta {}. Average contribution {} and average stopping time {}\".format(\n",
    "            theta, avg_contrib, avg_t_stop\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # obtain the theta values to carry out a full grid search\n",
    "    grid_search_theta_values = P.grid_search_theta_values(\n",
    "        sheet2[\"low_min\"],\n",
    "        sheet2[\"low_max\"],\n",
    "        sheet2[\"high_min\"],\n",
    "        sheet2[\"high_max\"],\n",
    "        sheet2[\"increment_size\"],\n",
    "    )\n",
    "    # use those theta values to calculate corresponding contribution values\n",
    "\n",
    "    contribution_iterations = [\n",
    "        P.vary_theta(\n",
    "            param_list, policy_info, \"high_low\", t, grid_search_theta_values[0]\n",
    "        )\n",
    "        for ite in list(range(nIterations))\n",
    "    ]\n",
    "\n",
    "    contribution_iterations_arr = np.array(contribution_iterations)\n",
    "    cum_sum_contrib = contribution_iterations_arr.cumsum(axis=0)\n",
    "    nElem = np.arange(1, cum_sum_contrib.shape[0] + 1).reshape(\n",
    "        (cum_sum_contrib.shape[0], 1)\n",
    "    )\n",
    "    cum_avg_contrib = cum_sum_contrib / nElem\n",
    "    print(\"cum_avg_contrib\")\n",
    "    print(cum_avg_contrib)\n",
    "\n",
    "    # plot those contribution values on a heat map\n",
    "    P.plot_heat_map_many(\n",
    "        cum_avg_contrib,\n",
    "        grid_search_theta_values[1],\n",
    "        grid_search_theta_values[2],\n",
    "        printIterations,\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "print(\"{} secs\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
